# Projecte final de grau superior d'ASIX
![1-kubernetes_logo](./arxius/imatges/1-kubernetes_logo.png)

---
---
## IntroducciÃ³ a Kubernetes 

### QuÃ¨ Ã©s Kubernetes?  
  
Kubernetes Ã©s un sistema de *software* de codi obert que serveix per a automatitzar la implementaciÃ³ i gestiÃ³ de sistemes d'aplicaciÃ³ complexos a gran escala, compostos per processos informÃ tics que s'executen en *contaniers*.  
  
Kubernetes permet executar aplicacions de *software* en milers de nodes d'ordinadors  
com si tots aquests nodes fossin un de molt gra. AixÃ² permet que s'abstregui la infraestructura subjacent i fent que se simplifiqui el desenvolupament, el desplegament i la gestiÃ³.

### Quin Ã©s l'origen de Kubernetes?  
  
Kubernetes va ser desenvolupat originalment per Google, que sempre ha executat  
aplicacions en contenidors. Ja en 2014 van dir que llanÃ§aven 2.000 milions de contenidors cada setmana. AixÃ² Ã©s mÃ©s de 3.000 contenidors per segon, i la xifra Ã©s molt  
mÃ©s alta avui. Fan funcionar aquests contenidors en milers d'ordinadors distribuÃ¯ts a travÃ©s  
dotzenes de centres de dades a tot el mÃ³n.  
  
Per aixÃ² van crear Kubernetes, perquÃ¨ va sorgir la necessitat d'automatitzar totes les tasques d'arrencada, manteniment i actualitzaciÃ³ de contenidors a escala massiva.

### D'on ve el nom Kubernetes?  
  
La paraula Kubernetes ve del grec i en aquest idioma vol dir timoner, o sigui, la persona que porta el timÃ³ d'un vaixell. Un timoner no Ã©s necessÃ riament el mateix que un  
capitÃ . Un capitÃ  Ã©s responsable del vaixell, mentre que el timoner Ã©s qui el dirigeix.  
  
El timoner mantÃ© el curs del vaixell, porta a terme les ordres donades pel capitÃ  i li reporta el transcurs de la nau. Kubernetes dirigeix les aplicacions i reporta sobre el seu estat mentre tu - el capitÃ  - decideixes on vol que vagi el sistema.

![2-timoner](./arxius/imatges/2-timoner.jpg)

### QuÃ¨ fa Kubernetes?  
  
Podem visualitzar Kubernetes com un sistema construÃ¯t en capes, amb cada capa superior abstraient la complexitat que es troba en les capes mÃ©s baixes.  
  
A la base, Kubernetes reuneix les mÃ quines fÃ­siques o virtuals individuals en un clÃºster utilitzant una xarxa compartida per comunicar-se entre cada ordinador. Aquest clÃºster de Kubernetes Ã©s la plataforma fÃ­sica on es configuren tots els components, capacitats i cÃ rregues de treball de Kubernetes.

![3-kub_capes](./arxius/imatges/3-kub_capes.PNG)
 
Quan es desplega una aplicaciÃ³ a travÃ©s de Kubernetes, automÃ ticament es selecciona un ordinador per a cada component de l'aplicaciÃ³, es desplega i aixÃ² li permet trobar i comunicar-se fÃ cilment amb mÃ©s components i amb altres aplicacions, entre altres funcions.

### Com s'estructura Kubernetes?

Cada ordinador que treballa sota el clÃºster de Kubernetes se'l coneix com a node.

Depenent de la funciÃ³ que hagi de complir el node, aquest es pot categoritzar en *master node* o en *worker node*.

+ Els *master nodes* treballen en el que es coneix com a *control plane*. El *control plane* Ã©s el component de Kubernetes que actua com a porta d'entrada i com a cervell del clÃºster, exposa l'API de Kubernetes per a usuaris i clients, comprova la salut d'altres nodes, decideix com dividir i assignar processos i orquestra la comunicaciÃ³ entre altres components.Els *master nodes* actuen com el punt principal de contacte amb el clÃºster i sÃ³n els responsables de la major part de la lÃ²gica centralitzada que proporciona Kubernetes.

+ Els *worker nodes* treballen en el que es coneix com a *workload plane*. El *workload plane* Ã©s el component de Kubernetes que executa les aplicacions en contenidors i Ã©s l'encarregat d'executar, controlar i proporcionar serveis a les aplicacions.

![4-cwplanes](./arxius/imatges/4-cwplanes.PNG)
![4-cwplanes2](./arxius/imatges/4-cwplanes2.PNG)

#### Quins sÃ³n els components dels nodes?

Tots dos tipus de nodes es caracteritzen per tenir una sÃ¨rie de components que els permet complir el seu treball. Hi ha components Ãºnics per tipus de node:

+ Components dâ€™un Master Node
  + etcd -> Servei que sâ€™encarrega dâ€™emmagatzemar informaciÃ³ del clÃºster accesible per tots els nodes.
  + kube-apiserver -> Servidor API que permet a lâ€™usuari les cÃ rregues de treball de Kubernetes entre altres funcions.
  + kube-controller-manager -> Servei que regula lâ€™estat del clÃºster, gestiona els cicles de vida de les cÃ rregues de treball i realitza tasques rutinÃ ries.
  + kube-schedluer -> Servei que sâ€™encarrega dâ€™assignar cÃ rregues de treball a nodes especifics.

+ Components dâ€™un Worker Node
  + containers runtime -> *Software* per treballar amb contenidors, com per exemple Docker.
  + kubelet -> Servei que fa de punt de contacte del node amb el *control plane*.
  + kube-proxy-> Servei que sâ€™encarrega dâ€™enrutar peticions dels contenidors a la xarxa interna del node.

### Com es llanÃ§a una aplicaciÃ³ en Kubernetes?

Per llanÃ§ar una aplicaciÃ³ en Kubernetes, primer s'ha de contenidoritzar l'aplicaciÃ³ i crear una imatge. DesprÃ©s de crear-la, s'ha de donar a l'API de Kubernetes una descripciÃ³ de l'aplicaciÃ³.

La descripciÃ³ inclou informaciÃ³ com la imatge o imatges dels components, com es relacionen entre ells, quantes cÃ²pies ha d'haver-hi, quin nom han de tenir, etc.

Quan l'API processa la descripciÃ³ de l'aplicaciÃ³, Kubernetes crea objectes a partir de les imatges de contenidors i els assigna als nodes de treball disponibles.

![5-kubernetes_work](./arxius/imatges/5-kubernetes_work.PNG)

---

## Interactuar amb Kubernetes

### QuÃ¨ Ã©s Minikube?

Minikube Ã©s una distribuciÃ³ reduÃ¯da de Kubernetes que permet muntar un clÃºster amb nomÃ©s un node.

Per fer aquest projecte he utilitzat Minikube, ja que Ã©s l'eina mÃ©s fÃ cil per aprendre a interactuar amb l'API sense haver de disposar de moltes mÃ quines fÃ­siques o virtuals configurades.

Per instalÂ·lar Minikube, es pot fer servir l'enllaÃ§ a la segÃ¼ent pÃ gina web: <https://k8s-docs.netlify.app/en/docs/tasks/tools/install-minikube/>

DesprÃ©s d'instalÂ·lar Minikube, el podem iniciar amb la segÃ¼ent comanda:

`minikube start`

```
a184311jq@PC:~/kubernetes$ minikube start
ğŸ˜„  minikube v1.30.1 en Ubuntu 23.04 (vbox/amd64)
âœ¨  Controlador docker seleccionado automÃ¡ticamente
ğŸ“Œ  Using Docker driver with root privileges
ğŸ‘  Starting control plane node minikube in cluster minikube
ğŸšœ  Pulling base image ...
ğŸ”¥  Creando docker container (CPUs=2, Memory=2200MB) ...
ğŸ³  Preparando Kubernetes v1.26.3 en Docker 23.0.2...
    â–ª Generando certificados y llaves
    â–ª Iniciando plano de control
    â–ª Configurando reglas RBAC...
ğŸ”—  Configurando CNI bridge CNI ...
    â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
ğŸ”  Verifying Kubernetes components...
ğŸŒŸ  Complementos habilitados: default-storageclass, storage-provisioner
ğŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
```

Un cop estÃ  Minikube en marxa, podem fer la segÃ¼ent comanda per veure el seu estat:

`minikube status`

```
a184311jq@PC:~/kubernetes$ minikube status
minikube
type: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured

```

Per veure el llistat de nodes del clÃºster (de moment nomÃ©s Ã©s un) i de les seves IPs hem d'utilitzar la segÃ¼ent comanda:

`minikube nodes list`

Si nomÃ©s volem veure la IP del node, hem d'utilitzar la segÃ¼ent comanda:

`minikube ip`

```
a184311jq@PC:~/kubernetes$ minikube node list
minikube	192.168.49.2
a184311jq@PC:~/kubernetes$ minikube ip
192.168.49.2
```

Finalment, aturem Minikube amb la segÃ¼ent comanda:

`minikube stop`

```
a184311jq@PC:~/kubernetes$ minikube stop
âœ‹  Stopping node "minikube"  ...
ğŸ›‘  Apagando "minikube" mediante SSH...
ğŸ›‘  1 node stopped.
```

El que fa en realitat Minikube Ã©s muntar una mÃ quina virtual que actuarÃ  com Ãºnic node i on sÃ³n tots els components d'un Master node, tot i que tambÃ© actua com un Worker node.

![6-mk_vm](./arxius/imatges/6-mk_vm.PNG)

PerÃ² amb Minikube tambÃ© podem crear un clÃºster amb dos nodes o mÃ©s amb la segÃ¼ent comanda:

`minikube start --node 2`

```
a184311jq@PC~/kubernetes$ minikube start --nodes 2
ğŸ˜„  minikube v1.30.1 en Ubuntu 23.04 (vbox/amd64)
âœ¨  Controlador docker seleccionado automÃ¡ticamente
ğŸ“Œ  Using Docker driver with root privileges
ğŸ‘  Starting control plane node minikube in cluster minikube
ğŸšœ  Pulling base image ...
ğŸ”¥  Creando docker container (CPUs=2, Memory=2200MB) ...
ğŸ³  Preparando Kubernetes v1.26.3 en Docker 23.0.2...
    â–ª Generando certificados y llaves
    â–ª Iniciando plano de control
    â–ª Configurando reglas RBAC...
ğŸ”—  Configurando CNI CNI ...
    â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
ğŸ”  Verifying Kubernetes components...
ğŸŒŸ  Complementos habilitados: storage-provisioner, default-storageclass

ğŸ‘  Starting worker node minikube-m02 in cluster minikube
ğŸšœ  Pulling base image ...
ğŸ”¥  Creando docker container (CPUs=2, Memory=2200MB) ...

ğŸ§¯  Docker is nearly out of disk space, which may cause deployments to fail! (88% of capacity). You can pass '--force' to skip this check.
ğŸ’¡  Suggestion: 

    Try one or more of the following to free up space on the device:
    
    1. Run "docker system prune" to remove unused Docker data (optionally with "-a")
    2. Increase the storage allocated to Docker for Desktop by clicking on:
    Docker icon > Preferences > Resources > Disk Image Size
    3. Run "minikube ssh -- docker system prune" if using the Docker container runtime
ğŸ¿  Related issue: https://github.com/kubernetes/minikube/issues/9024

ğŸŒ  Se han encontrado las siguientes opciones de red:
    â–ª NO_PROXY=192.168.49.2
ğŸ³  Preparando Kubernetes v1.26.3 en Docker 23.0.2...
    â–ª env NO_PROXY=192.168.49.2
ğŸ”  Verifying Kubernetes components...
ğŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
```

Si executem la comanda `minikube node list` per veure els nodes, ara podem veure que hi apareixen dues entrades. TambÃ© executem la comanda `minikube status` per veure'n l'estat dels dos nodes. 

```
a184311jq@PC:~/kubernetes$ minikube node list
minikube	192.168.49.2
minikube-m02	192.168.49.3
a184311jq@PC:~/kubernetes$ minikube status
minikube
type: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured

minikube-m02
type: Worker
host: Running
kubelet: Running

```
Aturem el clÃºster:

```
a184311jq@PC:~/kubernetes$ minikube stop
âœ‹  Stopping node "minikube"  ...
ğŸ›‘  Apagando "minikube" mediante SSH...
âœ‹  Stopping node "minikube-m02"  ...
ğŸ›‘  Apagando "minikube-m02" mediante SSH...
ğŸ›‘  2 nodes stopped.
```

---

### QuÃ¨ Ã©s `kubectl` ?

'kubectl' Ã©s l'eina de CLI que ofereix Kubernetes perquÃ¨ els usuaris interactuÃ¯n amb el Control Plane mitjanÃ§ant l'API de Kubernetes.

Per instalÂ·lar 'kubectl', es pot fer servir l'enllaÃ§ a la segÃ¼ent pÃ gina web: <https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/>

Un cop instalÂ·lat i amb el clÃºster de Minikube encÃ¨s, podem comenÃ§ar a interactuar amb l'API.

Si volem obtenir informaciÃ³ del clÃºster, hem d'executar la segÃ¼ent comanda:

`kubectl cluster-info`

```
a184311jq@PC:~$ kubectl cluster-info
Kubernetes control plane is running at https://192.168.49.2:8443
CoreDNS is running at https://192.168.49.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
```

Si volem llistar els nodes del clÃºster, hem d'executar la segÃ¼ent comanda:

`kubectl get nodes`

```
a184311jq@PC:~$ kubectl get nodes
NAME   	STATUS   ROLES       	AGE	VERSION
minikube   Ready	control-plane   168m   v1.26.3
```

Totes les ordres per interactuar amb l'API de Kubernetes es poden fer per CLI, perÃ² per crear objectes de l'API es pot fer tambÃ© mitjanÃ§ant arxius de configuraciÃ³.

Per aquest treball, tots els exemples s'han fet mitjanÃ§ant arxius YAML.


#### Quines sÃ³n les caracterÃ­stiques dâ€™un arxiu YAML dâ€™objecte API Kubernetes?

Lâ€™API de Kubernetes disposa de diversos objectes que veurem a continuaciÃ³, i els arxius YAML tenen petites diferencies entre cada objecte, perÃ² tots tenen certes parts en comÃº.

+ apiVersion:   -> Camp on es diu la versiÃ³ de l'API de Kubernetes utilitzada per lâ€™objecte.
+ kind: -> Camp on es diu el tipus dâ€™objecte de l'API de Kubernetes.
+ metadata: -> Camp on sâ€™especifica el nom de lâ€™objecte, etiquetes, anotacions, etc.
+ spec: -> Camp on es diu la imatge del contenidor, els ports, etc.

MÃ©s endavant veurem els diferents arxius per cada objecte.

---

### Quins sÃ³n els objectes de l'API de Kubernetes?

Mentre que els contenidors sÃ³n el mecanisme utilitzat per desplegar aplicacions, Kubernetes utilitza capes addicionals d'abstracciÃ³ sobre la interfÃ­cie del contenidor per proporcionar escala, resiliÃ¨ncia i funcions de gestiÃ³ del cicle de vida. En lloc de gestionar els contenidors directament, l'usuari defineix i interactua amb instÃ ncies compostes de diversos objectes de Kubernetes.

Per poder treballar amb els diferents objectes de l'API de Kubernetes, he fet servir una app simple en Javascript que la seva funciÃ³ Ã©s retornar el *hostname* de la mÃ quina que estÃ  executant l'aplicaciÃ³.

> [app.js](./arxius/app/app-base/app.js)

```
const http = require('http');
const os = require('os');
console.log("App server starting...");
var handler = function(request, response) {
console.log("Received request from " + request.connection.remoteAddress);
response.writeHead(200);
response.end("You've hit " + os.hostname() + "\n");
};
var www = http.createServer(handler);
www.listen(8080);
```

---

#### QuÃ¨ Ã©s un *pod*?

El *pod* Ã©s la unitat mÃ©s petita amb quÃ¨ treballa Kubernetes. Kubernetes no treballa amb els contenidors de forma directa, sinÃ³ que ho fa sobre els *pods* que Ã©s on s'estan executant els contenidors.

Les caracterÃ­stiques d'un *pod* sÃ³n que sempre tenen un o mÃ©s contenidors, mai poden estar compartits entre nodes i cadascun es comporta com una mÃ quina separada amb la seva prÃ²pia IP, *hostname*, processos, etc. executant una Ãºnica aplicaciÃ³.

![7-pods](./arxius/imatges/7-pods.PNG)

Normalment els *pods* no es creen de forma manual, perÃ² per posar un exemple, crearÃ© un *pod* que executi la segÃ¼ent [app](./arxius/app/app-base/app.js).

Primer de tot, hem de crear l'arxiu YAML de l'objecte *pod*.

> [app-manual.yaml](./arxius/pods/app-manual.yaml)

```
apiVersion: v1
kind: Pod
metadata:
  name: app-manual
spec:
  containers:
  - image: jordiiqb/app
    name: app
    ports:
    - containerPort: 8080
      protocol: TCP
```

En aquest cas, en l'apartat `kind: ` hem d'especificar que volem crear un objecte de tipus `Pod`.

A l'apartat `metadata` especifiquem quin serÃ  el nom que tindrÃ  el nostre objecte en el clÃºster. En aquest cas es dirÃ  `app-manual`.

I per Ãºltim, en el subapartat `containers: ` dins l'apartat `spec: ` especifiquem d'on treu la imatge del contenidor, quin nom tindrÃ  i per quin port escoltarÃ .

Per crear l'objecte *pod* en Kubernetes, utiltzem la segÃ¼ent comanda:

`kubectl create -f app-manual.yaml`

```
a184311jq@PC:~/kubernetes/arxius/pods$ kubectl create -f app-manual.yaml 
pod/app-manual created
```

Alternativament, tambÃ© podem utilitzar aquesta comanda per crear l'objecte si no estÃ  creat o per modificar-ho si ho estÃ .


```
a184311jq@PC:~/kubernetes/arxius/pods$ kubectl apply -f app-manual.yaml 
pod/app-manual created
```

Per poder llistar els *pods*, utiltzem la segÃ¼ent comanda:

`kubectl get pods`

```
a184311jq@PC:~/kubernetes/arxius/pods$ kubectl get pods
NAME         READY   STATUS    RESTARTS   AGE
app-manual   1/1     Running   0          84s
```

Per llistar-los amb mÃ©s informaciÃ³, utiltzem la segÃ¼ent comanda:

`kubectl get pods -o wide`

```
a184311jq@PC:~/kubernetes/arxius/pods$ kubectl get pods -o wide
NAME         READY   STATUS    RESTARTS   AGE    IP           NODE       NOMINATED NODE   READINESS GATES
app-manual   1/1     Running   0          106s   10.244.0.8   minikube   <none>           <none>
```

Amb l'Ãºltima ordre podem veure la IP asignada del *pod*. Si fem un `curl` a la IP (10.244.0.8) en el port 8080, ens hauria de retornar el nom de host del *pod*, perÃ² si ho executem veiem que no Ã©s aixÃ­:

```
a184311jq@PC:~/kubernetes/arxius/pods$ curl -s 10.244.0.8:8080




```

Els *pods* nomÃ©s son visibles dins del propi clÃºster de Kubernetes. Per poder accedir al recurs dins d'un *pod*, en aquest cas ho podem fer de la segÃ¼ent forma:

`kubectl port-forward app-manual 8888:8080`

```
a184311jq@PC:~/kubernetes/arxius/pods$ kubectl port-forward app-manual 8888:8080
Forwarding from 127.0.0.1:8888 -> 8080
Forwarding from [::1]:8888 -> 8080
Handling connection for 8888


```
```
a184311jq@PC:~/kubernetes/arxius/pods$ curl -s localhost:8888
You've hit app-manual
```

La comanda `kubectl port-forward` enruta el trÃ fic del port 8080 del *pod* a un port de la nostra mÃ quina, en aquest cas el 8888.

Per eliminar un *pod*, utilitzem la segÃ¼ent comanda:

`kubectl delete pod app-manual`

```
a184311jq@PC:~/kubernetes/arxius/pods$ kubectl delete pod app-manual
pod "app-manual" deleted
```

---

#### QuÃ¨ sÃ³n un *replication controller* i un *replication set*?

Els *pods* sÃ³n objectes que es van pensar per ser efÃ­mers. A vegades un *pod* pot fallar degut a un bug o un problema propi del node i no Ã©s viable haber d'estar aixecant manualment tots el *pods* del clÃºster.

Per aixÃ² es van idear els *replication controllers*. Aquests sÃ³n uns objectes de l'API quan se'ls dona una definiciÃ³ s'encarreguen de mantenir un nÃºmero de *pods* executant-se en tot moment.
Si un *pod* deixa de funcionar, el Replica Controller s'encarregarÃ  d'aixecar-ne un de nou.

![8-rcs](./arxius/imatges/8-rcs.png)

Veiem un exemple d'arxiu YAML d'un objecte *replication controller*.

> [app-rc.yaml](./arxius/replication_controllers/app-rc.yaml)

```
apiVersion: v1
kind: ReplicationController
metadata:
  name: app-rc
spec:
  replicas: 3
  selector:
    app: app
  template:
    metadata:
      labels:
        app: app
    spec:
      containers:
      - name : app
        image: jordiiqb/app
        ports:
        - containerPort: 8080
```

En aquest arxiu veiem tres subapartats nous de l'apartat `spec:` :

+ `replicas: `
+ `selector: `
+ `template: `

En el subapartat `replicas: ` especifiquem quantes cÃ²pies del *pod* volem.
En el subapartat `selector: ` especifiquem quin Ã©s l'etiqueta per la qual revisarÃ  els *pods* concrets.
En el subapartat `template: ` especifiquem com serÃ  la "plantilla" dels nostres *pods*.

Aquest Ã©s el bucle que farÃ­a el meu *replication controller* "app-rc" per verificar si hi ha 3 *pods* "app":

![9-rc_selector](./arxius/imatges/9-rc_selector.png)

Per crear l'objecte *replication controller* en Kubernetes, utiltzem la segÃ¼ent comanda:

`kubectl create -f app-rc.yaml`
```
a184311jq@PC:~/kubernetes/arxius/replication_controllers$ kubectl create -f app-rc.yaml 
replicationcontroller/app-rc created
```

Un cop creat el *replication controller*, podem verificar el seu estat amb la segÃ¼ent ordre:

`kubectl get rc`
```
a184311jq@PC:~/kubernetes/arxius/replication_controllers$ kubectl get rc
NAME     DESIRED   CURRENT   READY   AGE
app-rc   3         3         3       3m40s
```

Si llistem els *pods* del clÃºster, veiem que s'han creat tres objectes de tipus *pod*:

```
a184311jq@PC:~/kubernetes/arxius/replication_controllers$ kubectl get pods
NAME           READY   STATUS    RESTARTS   AGE
app-rc-8dzcp   1/1     Running   0          71s
app-rc-lvbjc   1/1     Running   0          71s
app-rc-tx8tp   1/1     Running   0          71s
```

Si eliminem un *pod* qualsevol, podem veure com el *replication controller* s'encarrega de generar-ne un de nou.

```
a184311jq@PC:~/kubernetes/arxius/replication_controllers$ kubectl get pods
NAME           READY   STATUS    RESTARTS   AGE
app-rc-8dzcp   1/1     Running   0          6m20s
app-rc-lvbjc   1/1     Running   0          6m20s
app-rc-tx8tp   1/1     Running   0          6m20s

a184311jq@PC:~/kubernetes/arxius/replication_controllers$ kubectl delete pod app-rc-8dzcp
pod "app-rc-8dzcp" deleted

a184311jq@PC:~/kubernetes/arxius/replication_controllers$ kubectl get pods
NAME           READY   STATUS        RESTARTS   AGE
app-rc-8dzcp   1/1     Terminating   0          6m50s
app-rc-lvbjc   1/1     Running       0          6m50s
app-rc-tx8tp   1/1     Running       0          6m50s
app-rc-vgq9h   1/1     Running       0          17s
```

En el cas que vulguem crear mÃ©s rÃ¨pliques o volguem modificar algÃºn parÃ metre de l'objecte ja creat, utilitzarem la segÃ¼ent comanda:

`kubectl edit rc app-rc`

Aquesta comanda ens obre un editor de text que ens deixa modificar les propietats de l'objecte *replication controller* ja creat.

```
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
kind: ReplicationController
metadata:
  creationTimestamp: "2023-06-05T22:11:34Z"
  generation: 1
  labels:
    app: app
  name: app-rc
  namespace: default
  resourceVersion: "11930"
  uid: d0ef54cc-a4ed-4588-91d2-a405d3a2c175
spec:
  replicas: 3
  selector:
    app: app
  template:
    metadata:
      creationTimestamp: null
	  labels:
        app: app
    spec:
      containers:
      - image: jordiiqb/app
        imagePullPolicy: Always
        name: app
        ports:
        - containerPort: 8080
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
	  restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 3
  fullyLabeledReplicas: 3
  observedGeneration: 1
  readyReplicas: 3
  replicas: 3
```

Al canviar el nÃºmero de rÃ¨pliques de 3 a 5 en l'apartat `spec:  replicas: 3`, podem veure que s'han creat dos *pods* mÃ©s:

```
a184311jq@PC:~/kubernetes/arxius/replication_controllers$ kubectl get pods
NAME           READY   STATUS    RESTARTS   AGE
app-rc-j6dlf   1/1     Running   0          2m46s
app-rc-lvbjc   1/1     Running   0          22m
app-rc-tnwh8   1/1     Running   0          2m46s
app-rc-tx8tp   1/1     Running   0          22m
app-rc-vgq9h   1/1     Running   0          16m
```

Alternativament, es podria fer amb la segÃ¼ent comanda:

`kubectl scale rc app-rc --replicas 6`

```
a184311jq@PC:~/kubernetes/arxius/replication_controllers$ kubectl scale rc app-rc --replicas 6
replicationcontroller/app-rc scaled
a184311jq@PC:~/kubernetes/arxius/replication_controllers$ kubectl get pods

NAME           READY   STATUS    RESTARTS   AGE
app-rc-j6dlf   1/1     Running   0          3m31s
app-rc-lvbjc   1/1     Running   0          23m
app-rc-tnwh8   1/1     Running   0          3m31s
app-rc-tx8tp   1/1     Running   0          23m
app-rc-vgq9h   1/1     Running   0          16m
app-rc-z98z5   1/1     Running   0          9s

```

Si eliminem el *replication controller*, podem veure que automÃ ticament s'eliminen tots el *pods* que s'havien creat:

```
a184311jq@PC:~/kubernetes/arxius/replication_controllers$ kubectl delete rc app-rc
replicationcontroller "app-rc" deleted
a184311jq@PC:~/kubernetes/arxius/replication_controllers$ kubectl get pods
NAME           READY   STATUS        RESTARTS   AGE
app-rc-j6dlf   1/1     Terminating   0          6m54s
app-rc-lvbjc   1/1     Terminating   0          26m
app-rc-tnwh8   1/1     Terminating   0          6m54s
app-rc-tx8tp   1/1     Terminating   0          26m
app-rc-vgq9h   1/1     Terminating   0          20m
app-rc-z98z5   1/1     Terminating   0          3m32s

```

En conclusiÃ³, tot i que els *replication controllers* sÃ³n molt Ãºtils, aquests estan limitats a l'hora de definir el camp `selector: ` ja que aquest nomÃ©s permet vigilar els *pods* que tenen una etiqueta igual al selector.
Els *replication sets* sÃ³n una evoluciÃ³ dels *replication controllers*, ja que permeten definir mÃ©s opcions al camp `selector: ` a part d'etiquetes. 

---

#### QuÃ¨ Ã©s un *service*?

Com hem vist anteriorment, hi ha diversos problemes a l'hora de treballar amb els *pods*. Hem vist que no podem accedir a un *pod* des de fora del clÃºster, perÃ² ara tambÃ© hem vist que no nomÃ©s hi ha un *pod*, si no que hi pot haver molts, aquests poden fallar i es poden crear de nous.
AixÃ² fa que no sigui viable treballar directament amb un *pod* per treballar contra una aplicaciÃ³, per aixÃ² es van crear els *services*.

L'objecte *service* Ã©s un objecte de l'API de Kubernetes que crea un punt d'entrada Ãºnic i constant per a tots els *pods* d'un mateix tipus, Ãºnificant-hi l'accÃ©s en un sol objecte.

Amb un objecte *service*, ens Ã©s indiferent a quin *pod* ataquem, ja que el propi objecte seleccionarÃ  un *pod* qualsevol de tots els que tÃ© enrutats.

Existeixen diferents tipus d'objectes *service*, perÃ² en aquest projecte en veurem dos:

+ ClusterIP
+ NodePort

L'objecte *service* de tipus Cluster IP Ã©s l'objecte *service* que es crea per defecte en Kubernetes i Ã©s el que nomÃ©s enruta cap a dins del propi clÃºster.

Per exemple, en la segÃ¼ent imatge, l'objecte "Frontend Service" Ã©s de tipus NodePort.

![10-services](./arxius/imatges/10-services.png)

En canvi, l'objecte *service* de tipus NodePort permet poder interactuar amb els *pods* des de fora del clÃºster, ja que el que fa Ã©s obrir un port estÃ tic en cada node del clÃºster i enruta el trÃ fic als *pods*.

Per exemple, en la segÃ¼ent imatge podem veure clarament com funciona un objecte *service* de tipus NodePort:

![11-svc_nd](./arxius/imatges/11-svc_nd.PNG)

Veiem un exemple d'arxiu YAML d'un objecte *service*.

> [app-svc.yaml](./arxius/services/app-svc.yaml)

```
apiVersion: v1
kind: Service
metadata:
  name: app-svc
spec:
  selector:
    app: app
  ports:
  - port: 80
    targetPort: 8080
```

D'igual forma que els *replication controllers* o els *replication sets*, els *services* utilitzen un selector amb una etiqueta per controlar els *pods* que han d'enrutar.

Per crear l'objecte *service* en Kubernetes, utiltzem la segÃ¼ent comanda:

`kubectl create -f app-svc.yaml`
```
a184311jq@PC:~/kubernetes/arxius/services$ kubectl create -f app-svc.yaml 
service/app-svc created
```
Si llistem tots el *services* mitjanÃ§ant la comanda `kubectl get services`, podem veure el nostre objecte *service* a mÃ©s d'un objecte que crea Minikube per defecte:

```
a184311jq@PC:~/kubernetes/arxius/services$ kubectl get services
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
app-svc      ClusterIP   10.107.252.192   <none>        80/TCP    15s
kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP   13h
```

Com en l'arxiu YAML no hem especificat quin tipus de *service* ha de ser l'objecte, per defecte el crearÃ  com a tipus ClusterIP.

Si utilitzem l'opciÃ³ `-o wide` podem veure que tambÃ© indica informaciÃ³ del selector:

```
a184311jq@PC:~/kubernetes/arxius/services$ kubectl get services -o wide
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE     SELECTOR
app-svc      ClusterIP   10.107.252.192   <none>        80/TCP    2m58s   app=app
kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP   13h     <none>
```

Eliminem l'objecte *service*:

`kubectl delete service app-svc`

```
a184311jq@PC:~/kubernetes/arxius/services$ kubectl delete service app-svc
service "app-svc" deleted
```

El nostre objectiu final Ã©s poder interactuar amb els *pods* des de fora del clÃºster, per aixÃ² necesitem crear un *service* de tipus NodePort.

Aquest Ã©s un exemple d'arxiu YAML d'un objecte *service* de tipus NodePort.

> [app-svc-nd.yaml](./arxius/services/app-svc-nd.yaml)

```
apiVersion: v1
kind: Service
metadata:
  name: app-svc-nodeport
spec:
  type: NodePort
  selector:
    app: app
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30123
```

En l'arxiu podem veure que hi ha dos camps nous en comparaciÃ³ amb el primer arxiu *service*. Podem veure que apareix el camp `type: NodePort` per especificar el tipus de *service* i a `ports: ` apareix un camp amb el port estÃ tic que obrirÃ  als nodes `nodePort: 30123`.

Creem l'objecte i el llistem:

```
a184311jq@PC:~/kubernetes/arxius/services$ kubectl create -f app-svc-np.yaml 
service/app-svc-nodeport created
a184311jq@PC:~/kubernetes/arxius/services$ kubectl get services
NAME               TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
app-svc-nodeport   NodePort    10.108.23.68   <none>        80:30123/TCP   39s
kubernetes         ClusterIP   10.96.0.1      <none>        443/TCP        14h
```

Podem veure a com ara el TYPE del *service* indica que Ã©s un NodePort i a l'apartat "PORT(S)" podem veure l'enrutaciÃ³ de ports del *service* al node.

Si amb els *pods* del *replication controller* encesos fem un `curl` a l'IP del node i al port 30123, podem veure que ja podem accedir a un *pod* del clÃºster des de fora:

```
a184311jq@PC:~/kubernetes/arxius/services$ kubectl get pods
NAME           READY   STATUS    RESTARTS   AGE
app-rc-cqh55   1/1     Running   0          4m24s
app-rc-g6nmp   1/1     Running   0          4m24s
app-rc-t7p4c   1/1     Running   0          4m24s
a184311jq@PC:~/kubernetes/arxius/services$ minikube ip
192.168.49.2
a184311jq@PC:~/kubernetes/arxius/services$ curl -s 192.168.49.2:30123
You've hit app-rc-cqh55
```

Si fem un parell de cops la comanda `curl` podem veure com el *service* selecciona un *pod* a l'atzar:

```
a184311jq@PC:~/kubernetes/arxius/services$ curl -s 192.168.49.2:30123
You've hit app-rc-cqh55
a184311jq@PC:~/kubernetes/arxius/services$ curl -s 192.168.49.2:30123
You've hit app-rc-g6nmp
a184311jq@PC:~/kubernetes/arxius/services$ curl -s 192.168.49.2:30123
You've hit app-rc-t7p4c
```
---

#### QuÃ¨ Ã©s un *deployment*?

Els *deployments* sÃ³n uns objectes de l'API de Kubernetes dissenyats per fer mÃ©s senzill el cicle de manteniment dels *pods* replicats.
Posem com a exemple aquest clÃºster:

![12-ex_cluster](./arxius/imatges/12-ex_cluster.PNG)

En el cas de voler actualitzar la imatge de la nostra aplicaciÃ³ de la versiÃ³ inicial (v1) a una nova versiÃ³ (v2) que s'utilitza com a plantilla en els *pods* en funcionament, haurÃ­em de modificar-ho al *replication controller*. Un cop aquest detecti el canvi, el que farÃ  serÃ  aturar tots els *pods* antics i els renovarÃ  amb la nova versiÃ³.
El problema de fer aixÃ² Ã©s que si l'aplicaciÃ³ que actualitzem ha d'estar sempre en funcionament, hi haurÃ  un petit perÃ­ode de temps en que no estarÃ  disponible.
TambÃ© pot passar que hi hagi algun problema amb la nova versiÃ³ i s'hagi de tornar a la versiÃ³ anterior, fent encara mÃ©s farragÃ³s tot el procÃ©s.

![13-upd1](./arxius/imatges/13-upd1.PNG)

Els *deployments* ens ajuden en aquest cas, ja que el que fan Ã©s crear i supervisar un *replication set* i uns *pods* amb la nova versiÃ³ de l'aplicaciÃ³ i progressivament van canviant els *pods* nous pels vells sense que l'aplicaciÃ³ deixi estar mai en funcionament.

![14-dep](./arxius/imatges/14-dep.png)

![15-upd2](./arxius/imatges/15-upd2.PNG)

Un cop ha acabat el procÃ©s d'actualitzaciÃ³, en el nostre clÃºster hem passat de l'estat de la imatge de l'esquerra per l'estat de la imatge de la dreta.

![16-estats_inicial_final](./arxius/imatges/16-estats_inicial_final.PNG)

Veiem un exemple d'arxiu YAML d'un objecte *replication controller*.

> [app-deployment.yaml](./arxius/deployments/app-deployment.yaml)

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: app
  template:
    metadata:
      labels:
        app: app
    spec:
      containers:
      - name : app
        image: jordiiqb/app
        ports:
        - containerPort: 8080
```

A simple vista, l'arxiu YAML Ã©s molt semblant a un arxiu YAML d'un *replication controller*, perÃ² en aquest cas, l'apartat selector estÃ  configurat amb un subapartat diferent, ja que els *deployments* treballen amb *replication sets* i no amb *replication controllers*.

Per crear l'objecte *deployment* en Kubernetes, utiltzem la segÃ¼ent comanda:

`kubectl create -f app-deployment.yaml`
```
a184311jq@PC:~/kubernetes/arxius/deployments$ kubectl create -f app-deployment.yaml 
deployment.apps/app-deployment created
```

Per veure l'estat del *deployment* podem fer servir la segÃ¼ent comanda:

`kubectl get deployment`

```
a184311jq@PC:~/kubernetes/arxius/deployments$ kubectl get deployment
NAME             READY   UP-TO-DATE   AVAILABLE   AGE
app-deployment   3/3     3            3           25s
```

PerÃ² si volem saber com ha anat el llanÃ§ament del *deployment*, podem executar la segÃ¼ent comanda:

`kubectl rollout status deployment app-deployment`

```
a184311jq@PC:~/kubernetes/arxius/deployments$ kubectl rollout status deployment app-deployment
deployment "app-deployment" successfully rolled out
```

Un cop sabem que el *deployment* s'ha creat correctament, podem revisar els *replication sets* i els *pods*:

```
a184311jq@PC:~/kubernetes/arxius/deployments$ kubectl get rs
NAME                        DESIRED   CURRENT   READY   AGE
app-deployment-547bcb94f8   3         3         3       34s
a184311jq@PC:~/kubernetes/arxius/deployments$ kubectl get pods
NAME                              READY   STATUS    RESTARTS   AGE
app-deployment-547bcb94f8-8bgrm   1/1     Running   0          39s
app-deployment-547bcb94f8-dn8mt   1/1     Running   0          39s
app-deployment-547bcb94f8-z46mv   1/1     Running   0          39s
```

Com encara tenim l'objecte *service* creat i les etiquetes dels *pods* sÃ³n les mateixes, podem intearctuar amb els *pods*:

```
a184311jq@PC:~/kubernetes/arxius/deployments$ curl -s 192.168.49.2:30123
You've hit app-deployment-547bcb94f8-z46mv
a184311jq@PC:~/kubernetes/arxius/deployments$ curl -s 192.168.49.2:30123
You've hit app-deployment-547bcb94f8-8bgrm
a184311jq@PC:~/kubernetes/arxius/deployments$ curl -s 192.168.49.2:30123
You've hit app-deployment-547bcb94f8-dn8mt
```

#### Prova d'actualitzaciÃ³ d'un *deployment*

Per veure les caracterÃ­stiques abans mencionades sobre els objectes *deployment*, he creat una imatge nova de l'app en Javascript:

> [app-v2.js](./arxius/app/app-v2/app-v2.js)

```
const http = require('http');
const os = require('os');
console.log("App server starting...");
var handler = function(request, response) {
console.log("Received request from " + request.connection.remoteAddress);
response.writeHead(200);
response.end("This is app v2 running. You've hit " + os.hostname() + "\n");
};
var www = http.createServer(handler);
www.listen(8080);
```

BÃ sicament, aquesta nova versiÃ³ retorna un *string* una mica diferent en comparaciÃ³ amb la versiÃ³ inicial.

Per veure com es comporten els *pods* mentres es fa l'update, executem la segÃ¼ent comanda per fer un bucle en un altre terminal:

`while true; do curl -s 192.168.49.2:30123; sleep 2; done`

Mentres deixem el bucle fent, revisem els *pods* i executem l'update amb la segÃ¼ent comanda:

`kubectl set-image deployment app-deployment nodejs=jordiiqb/app:v2`

```
a184311jq@PC:~/kubernetes/arxius/deployments$ kubectl get pods
NAME                              READY   STATUS    RESTARTS   AGE
app-deployment-6bbb7cffdf-9wp9s   1/1     Running   0          42s
app-deployment-6bbb7cffdf-btkcz   1/1     Running   0          45s
app-deployment-6bbb7cffdf-vlb94   1/1     Running   0          40s
a184311jq@PC:~/kubernetes/arxius/deployments$ kubectl set image deployment app-deployment app=jordiiqb/app:v2
deployment.apps/app-deployment image updated
```

Un cop executem l'update, si tornem a revisar els *pods*, veiem el segÃ¼ent:

```
a184311jq@PC:~/kubernetes/arxius/deployments$ kubectl get pods
NAME                              READY   STATUS              RESTARTS   AGE
app-deployment-56c9ff8d84-2slpw   1/1     Running             0          9s
app-deployment-56c9ff8d84-jdsbr   0/1     ContainerCreating   0          3s
app-deployment-56c9ff8d84-nf7r9   1/1     Running             0          6s
app-deployment-6bbb7cffdf-9wp9s   1/1     Running             0          78s
app-deployment-6bbb7cffdf-btkcz   1/1     Terminating         0          81s
app-deployment-6bbb7cffdf-vlb94   1/1     Terminating         0          76s
a184311jq@PC:~/kubernetes/arxius/deployments$ kubectl get pods
NAME                              READY   STATUS        RESTARTS   AGE
app-deployment-56c9ff8d84-2slpw   1/1     Running       0          19s
app-deployment-56c9ff8d84-jdsbr   1/1     Running       0          13s
app-deployment-56c9ff8d84-nf7r9   1/1     Running       0          16s
app-deployment-6bbb7cffdf-9wp9s   1/1     Terminating   0          88s
app-deployment-6bbb7cffdf-btkcz   1/1     Terminating   0          91s
app-deployment-6bbb7cffdf-vlb94   1/1     Terminating   0          86s
```

Veiem que en un moment ha creat els nous *pods* i ha eliminat els vells.

Si revisem el resultat del bucle, podem veure com ho ha fet progresivament:

```
You've hit app-deployment-6bbb7cffdf-9wp9s
You've hit app-deployment-6bbb7cffdf-btkcz
You've hit app-deployment-6bbb7cffdf-vlb94
You've hit app-deployment-6bbb7cffdf-9wp9s
You've hit app-deployment-6bbb7cffdf-vlb94
You've hit app-deployment-6bbb7cffdf-9wp9s
This is app v2 running. You've hit app-deployment-56c9ff8d84-2slpw
This is app v2 running. You've hit app-deployment-56c9ff8d84-nf7r9
This is app v2 running. You've hit app-deployment-56c9ff8d84-2slpw
This is app v2 running. You've hit app-deployment-56c9ff8d84-jdsbr
This is app v2 running. You've hit app-deployment-56c9ff8d84-jdsbr
This is app v2 running. You've hit app-deployment-56c9ff8d84-2slpw

```

Per fer els updates, tambÃ© podem modificar l'arxiu YAML i desprÃ©s executar la comanda `kubectl apply -f app-deployment.yaml` o tambÃ© podem executar `kubectl edit deployment app-deployment` i editar les propietats de l'objecte creat.

#### QuÃ¨ Ã©s un *namespace*?

Els objectes *namespaces* sÃ³n uns objectes de l'API de Kubernetes que permeten separar diferents grups de recursos dins d'un mateix clÃºster.

Com segurament sobre un clÃºster no hi treballarÃ  nomÃ©s una persona, si no varies persones o fins i tot dieferents departaments, Ã©s important crear una separaciÃ³ entre els objectes d'un tipus i d'un altre.
TambÃ© Ã©s important tenir en compte que Kubernetes no deixa crear mai dos objectes amb el mateix nom:

```
a184311jq@PC:~/kubernetes/arxius/pods$ kubectl create -f app-manual.yaml 
a184311jq@PC:~/kubernetes/arxius/pods$ kubectl get po
NAME         READY   STATUS    RESTARTS   AGE
app-manual   1/1     Running   0          66s
a184311jq@PC:~/kubernetes/arxius/pods$ kubectl create -f app-manual.yaml 
Error from server (AlreadyExists): error when creating "app-manual.yaml": pods "app-manual" already exists
```

Per defecte al arrencar el clÃºster de minikube, es crea el *namespace* `default` i aquÃ­ Ã©s on es creen quasi tots els objectes de l'API.
Minikube tambÃ© crea un *namespace* propi per executar tots els processos interns.

Podem veure els *namespaces* executant la segÃ¼ent ordre:

`kubectl get namespaces`
```
a184311jq@PC:~/kubernetes$ kubectl get namespaces
NAME              STATUS   AGE
default           Active   22h
kube-node-lease   Active   22h
kube-public       Active   22h
kube-system       Active   22h
```

Es poden crear *namespaces* diferents a `default`. Veiem un exemple d'arxiu YAML d'un objecte *namespace*:

> [custom-namespace.yaml](./arxius/namespaces/custom-namespace.yaml)

```
apiVersion: v1
kind: Namespace
metadata:
  name: jordi-namespace
```

Aquest arxiu YAML Ã©s molt mÃ©s simple que els anteriors.

Si executem la comanda `kubectl create -f custom-namespace.yaml` per crear el nostre *namespace* i desprÃ©s els llistem tots, podem veure que s'ha creat correctament.

```
a184311jq@PC:~/kubernetes/arxius/namespaces$ kubectl create -f custom-namespace.yaml 
namespace/jordi-namespace created
a184311jq@PC:~/kubernetes/arxius/namespaces$ kubectl get namespaces
NAME              STATUS   AGE
default           Active   23h
jordi-namespace   Active   2m3s
kube-node-lease   Active   23h
kube-public       Active   23h
kube-system       Active   23h
```

Ara ja podem crear una segona cÃ²pia del *pod* "app-manual" en el nostre *namespace* amb la segÃ¼ent comanda:

`kubectl create -f app-manual.yaml -n jordi-namespace`

```
a184311jq@PC:~/kubernetes/arxius/namespaces$ kubectl create -f ../pods/app-manual.yaml -n jordi-namespace
pod/app-manual created
a184311jq@PC:~/kubernetes/arxius/namespaces$ kubectl get po --namespace=jordi-namespace
NAME         READY   STATUS    RESTARTS   AGE
app-manual   1/1     Running   0          19s
```

Per llistar tots els *pods* de tot el clÃºster podem fer-ho amb la segÃ¼ent comanda:

`kubectl get po --all-namespaces`

```
a184311jq@PC:~/kubernetes/arxius/namespaces$ kubectl get po --all-namespaces
NAMESPACE         NAME                               READY   STATUS    RESTARTS      AGE
default           app-manual                         1/1     Running   0             14m
jordi-namespace   app-manual                         1/1     Running   0             2m11s
kube-system       coredns-787d4945fb-db56s           1/1     Running   6 (15m ago)   23h
kube-system       etcd-minikube                      1/1     Running   5 (15m ago)   23h
kube-system       kube-apiserver-minikube            1/1     Running   5 (15m ago)   23h
kube-system       kube-controller-manager-minikube   1/1     Running   5 (15m ago)   23h
kube-system       kube-proxy-wxrz2                   1/1     Running   5 (15m ago)   23h
kube-system       kube-scheduler-minikube            1/1     Running   5 (15m ago)   23h
kube-system       storage-provisioner                1/1     Running   8 (15m ago)   23h
```

Per poder executar totes les comandes en un namespace en concret si ja s'ha arrencat `minikube` amb el `default` *namespace*, executant la segÃ¼ent ordre podem canviar-ho:

`kubectl config set-context --current --namespace=jordi-namespace`

```
a184311jq@PC:~/kubernetes/arxius/namespaces$ kubectl config set-context --current --namespace=jordi-namespace
Context "minikube" modified.
```

Es pot verificar amb la segÃ¼ent ordre:

`kubectl config view | grep namespace`

```
a184311jq@PC:~/kubernetes/arxius/namespaces$ kubectl config view | grep namespace
    namespace: jordi-namespace

```

En el cas que vulgÃ¨ssim arrencar el clÃºster amb un *namespace* diferent a `default`, ho haurÃ­em de fer amb la segÃ¼ent comanda:

`minikube start --namespace=jordi-namespace`

```
a184311jq@PC:~/kubernetes/arxius/namespaces$ minikube start --namespace=jordi-namespace
ğŸ˜„  minikube v1.30.1 en Ubuntu 23.04 (vbox/amd64)
âœ¨  Using the docker driver based on existing profile
ğŸ‘  Starting control plane node minikube in cluster minikube
ğŸšœ  Pulling base image ...
ğŸ”„  Restarting existing docker container for "minikube" ...

ğŸ§¯  Docker is nearly out of disk space, which may cause deployments to fail! (92% of capacity). You can pass '--force' to skip this check.
ğŸ’¡  Suggestion: 

    Try one or more of the following to free up space on the device:
    
    1. Run "docker system prune" to remove unused Docker data (optionally with "-a")
    2. Increase the storage allocated to Docker for Desktop by clicking on:
    Docker icon > Preferences > Resources > Disk Image Size
    3. Run "minikube ssh -- docker system prune" if using the Docker container runtime
ğŸ¿  Related issue: https://github.com/kubernetes/minikube/issues/9024

ğŸ³  Preparando Kubernetes v1.26.3 en Docker 23.0.2...
ğŸ”—  Configurando CNI bridge CNI ...
ğŸ”  Verifying Kubernetes components...
    â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
ğŸŒŸ  Complementos habilitados: default-storageclass, storage-provisioner
ğŸ„  Done! kubectl is now configured to use "minikube" cluster and "jordi-namespace" namespace by default
```
